
Call:  (function (x, y, family = c("gaussian", "binomial", "poisson",      "multinomial", "cox", "mgaussian"), weights = NULL, offset = NULL,      alpha = 1, nlambda = 100, lambda.min.ratio = ifelse(nobs <          nvars, 0.01, 1e-04), lambda = NULL, standardize = TRUE,      intercept = TRUE, thresh = 1e-07, dfmax = nvars + 1, pmax = min(dfmax *          2 + 20, nvars), exclude = NULL, penalty.factor = rep(1,          nvars), lower.limits = -Inf, upper.limits = Inf, maxit = 1e+05,      type.gaussian = ifelse(nvars < 500, "covariance", "naive"),      type.logistic = c("Newton", "modified.Newton"), standardize.response = FALSE,      type.multinomial = c("ungrouped", "grouped"), relax = FALSE,      trace.it = 0, ...)  {     this.call = match.call()     np = dim(x)     if (is.null(np) | (np[2] <= 1))          stop("x should be a matrix with 2 or more columns")     nobs = as.integer(np[1])     nvars = as.integer(np[2])     if (any(is.na(x)))          stop("x has missing values; consider using makeX() to impute them")     if (is.null(weights))          weights = rep(1, nobs)     else if (length(weights) != nobs)          stop(paste("number of elements in weights (", length(weights),              ") not equal to the number of rows of x (", nobs,              ")", sep = ""))     if (is.function(exclude))          exclude <- check.exclude(exclude(x = x, y = y, weights = weights),              nvars)     if (length(penalty.factor) != nvars)          stop("the length of penalty.factor does not match the number of variables")     if (!is.character(family)) {         fit = glmnet.path(x, y, weights, lambda, nlambda, lambda.min.ratio,              alpha, offset, family, standardize, intercept, thresh = thresh,              maxit, penalty.factor, exclude, lower.limits, upper.limits,              trace.it = trace.it)         fit$call = this.call     }     else {         family = match.arg(family)         if (family == "cox" && use.cox.path(x, y)) {             fit <- cox.path(x, y, weights, offset, alpha, nlambda,                  lambda.min.ratio, lambda, standardize, thresh,                  exclude, penalty.factor, lower.limits, upper.limits,                  maxit, trace.it, ...)             fit$call <- this.call         }         else {             if (alpha > 1) {                 warning("alpha >1; set to 1")                 alpha = 1             }             if (alpha < 0) {                 warning("alpha<0; set to 0")                 alpha = 0             }             alpha = as.double(alpha)             nlam = as.integer(nlambda)             y = drop(y)             dimy = dim(y)             nrowy = ifelse(is.null(dimy), length(y), dimy[1])             if (nrowy != nobs)                  stop(paste("number of observations in y (", nrowy,                    ") not equal to the number of rows of x (",                    nobs, ")", sep = ""))             vnames = colnames(x)             if (is.null(vnames))                  vnames = paste("V", seq(nvars), sep = "")             ne = as.integer(dfmax)             nx = as.integer(pmax)             if (is.null(exclude))                  exclude = integer(0)             if (any(penalty.factor == Inf)) {                 exclude = c(exclude, seq(nvars)[penalty.factor ==                    Inf])                 exclude = sort(unique(exclude))             }             if (length(exclude) > 0) {                 jd = match(exclude, seq(nvars), 0)                 if (!all(jd > 0))                    stop("Some excluded variables out of range")                 penalty.factor[jd] = 1                 jd = as.integer(c(length(jd), jd))             }             else jd = as.integer(0)             vp = as.double(penalty.factor)             internal.parms = glmnet.control()             if (internal.parms$itrace)                  trace.it = 1             else {                 if (trace.it) {                   glmnet.control(itrace = 1)                   on.exit(glmnet.control(itrace = 0))                 }             }             if (any(lower.limits > 0)) {                 stop("Lower limits should be non-positive")             }             if (any(upper.limits < 0)) {                 stop("Upper limits should be non-negative")             }             lower.limits[lower.limits == -Inf] = -internal.parms$big             upper.limits[upper.limits == Inf] = internal.parms$big             if (length(lower.limits) < nvars) {                 if (length(lower.limits) == 1)                    lower.limits = rep(lower.limits, nvars)                 else stop("Require length 1 or nvars lower.limits")             }             else lower.limits = lower.limits[seq(nvars)]             if (length(upper.limits) < nvars) {                 if (length(upper.limits) == 1)                    upper.limits = rep(upper.limits, nvars)                 else stop("Require length 1 or nvars upper.limits")             }             else upper.limits = upper.limits[seq(nvars)]             cl = rbind(lower.limits, upper.limits)             if (any(cl == 0)) {                 fdev = glmnet.control()$fdev                 if (fdev != 0) {                   glmnet.control(fdev = 0)                   on.exit(glmnet.control(fdev = fdev))                 }             }             storage.mode(cl) = "double"             isd = as.integer(standardize)             intr = as.integer(intercept)             if (!missing(intercept) && family == "cox")                  warning("Cox model has no intercept")             jsd = as.integer(standardize.response)             thresh = as.double(thresh)             if (is.null(lambda)) {                 if (lambda.min.ratio >= 1)                    stop("lambda.min.ratio should be less than 1")                 flmin = as.double(lambda.min.ratio)                 ulam = double(1)             }             else {                 flmin = as.double(1)                 if (any(lambda < 0))                    stop("lambdas should be non-negative")                 ulam = as.double(rev(sort(lambda)))                 nlam = as.integer(length(lambda))             }             is.sparse = FALSE             ix = jx = NULL             if (inherits(x, "sparseMatrix")) {                 is.sparse = TRUE                 x = as(x, "CsparseMatrix")                 x = as(x, "dMatrix")                 ix = as.integer(x@p + 1)                 jx = as.integer(x@i + 1)                 if (family != "cox") {                   xd <- x                 }                 else {                   xd <- x@x                 }             }             else if (!inherits(x, "matrix")) {                 xd <- data.matrix(x)             }             else {                 xd <- x             }             if (!inherits(xd, "sparseMatrix")) {                 storage.mode(xd) <- "double"             }             if (trace.it) {                 if (relax)                    cat("Training Fit\n")                 pb <- createPB(min = 0, max = nlam, initial = 0,                    style = 3)             }             else {                 pb <- NULL             }             kopt = switch(match.arg(type.logistic), Newton = 0,                  modified.Newton = 1)             if (family == "multinomial") {                 type.multinomial = match.arg(type.multinomial)                 if (type.multinomial == "grouped")                    kopt = 2             }             kopt = as.integer(kopt)             fit = switch(family, gaussian = elnet(xd, is.sparse,                  y, weights, offset, type.gaussian, alpha, nobs,                  nvars, jd, vp, cl, ne, nx, nlam, flmin, ulam,                  thresh, isd, intr, vnames, maxit, pb), poisson = fishnet(xd,                  is.sparse, y, weights, offset, alpha, nobs, nvars,                  jd, vp, cl, ne, nx, nlam, flmin, ulam, thresh,                  isd, intr, vnames, maxit, pb), binomial = lognet(xd,                  is.sparse, ix, jx, y, weights, offset, alpha,                  nobs, nvars, jd, vp, cl, ne, nx, nlam, flmin,                  ulam, thresh, isd, intr, vnames, maxit, kopt,                  family, pb), multinomial = lognet(xd, is.sparse,                  ix, jx, y, weights, offset, alpha, nobs, nvars,                  jd, vp, cl, ne, nx, nlam, flmin, ulam, thresh,                  isd, intr, vnames, maxit, kopt, family, pb),                  cox = coxnet(xd, is.sparse, ix, jx, y, weights,                    offset, alpha, nobs, nvars, jd, vp, cl, ne,                    nx, nlam, flmin, ulam, thresh, isd, vnames,                    maxit), mgaussian = mrelnet(xd, is.sparse,                    y, weights, offset, alpha, nobs, nvars, jd,                    vp, cl, ne, nx, nlam, flmin, ulam, thresh,                    isd, jsd, intr, vnames, maxit, pb))             if (trace.it) {                 utils::setTxtProgressBar(pb, nlam)                 close(pb)             }             if (is.null(lambda))                  fit$lambda = fix.lam(fit$lambda)             fit$call = this.call             fit$nobs = nobs             class(fit) = c(class(fit), "glmnet")         }     }     if (relax)          relax.glmnet(fit, x = x, y = y, weights = weights, offset = offset,              lower.limits = lower.limits, upper.limits = upper.limits,              penalty.factor = penalty.factor, check.args = FALSE,              ...)     else fit })(x = structure(c(88, 57, 68, 70, 73, 60, 65, 70, 64, 68, 69,  33, 50, 65, 69, 59, 64, 61, 73, 47, 61, 62, 67, 66, 59, 71, 64,  93, 77, 54, 70, 63, 64, 63, 54, 43, 65, 56, 70, 70, 52, 66, 61,  63, 69, 80, 75, 79, 61, 70, 70, 52, 60, 72, 54, 63, 58, 66, 65,  63, 63, 71, 69, 53, 73, 57, 62, 73, 69, 68, 65, 57, 62, 67, 50,  74, 80, 53, 54, 64, 53, 61, 69, 65, 64, 64, 55, 92, 55, 59, 72,  78, 66, 68, 61, 72, 74, 65, 62, 63, 60, 74, 68, 68, 67, 100,  74, 90, 77, 79, 81, 62, 83, 78, 74, 79, 70, 68, 82, 71, 83, 81,  86, 68, 66, 73, 56, 62, 61, 61, 62, 73, 83, 95, 96, 92, 63, 67,  65, 79, 67, 84, 66, 66, 88, 78, 73, 59, 70, 71, 69, 83, 69, 75,  61, 65, 71, 80, 73, 70, 67, 63, 73, 75, 65, 66, 63, 49, 92, 77,  63, 47, 75, 81, 70, 75, 58, 51, 65, 68, 63, 91, 63, 86, 72, 80,  65, 63, 68, 77, 66, 61, 65, 74, 57, 60, 76, 64, 64, 65, 56, 61,  57, 65, 43, 69, 61, 53, 57, 62, 52, 46, 60, 64, 61, 62, 73, 77,  61, 50, 66, 75, 52, 69, 89, 58, 70, 70, 75, 58, 70, 80, 73, 75,  69, 42, 63, 70, 69, 60, 68, 61, 70, 52, 57, 73, 77, 66, 68, 83,  66, 95, 75, 57, 70, 66, 69, 72, 62, 64, 71, 56, 68, 71, 63, 70,  54, 69, 66, 81, 82, 79, 61, 70, 77, 52, 63, 76, 60, 66, 68, 78,  70, 64, 66, 68, 72, 55, 76, 57, 81, 75, 88, 67, 72, 57, 70, 69,  52, 78, 85, 55, 58, 66, 42, 68, 67, 74, 70, 61, 57, 88, 70, 72,  79, 72, 70, 71, 69, 79, 74, 71, 67, 64, 65, 74, 71, 66, 71, 100,  74, 92, 78, 74, 83, 74, 87, 81, 81, 82, 73, 64, 75, 84, 81, 81,  87, 69, 68, 74, 65, 66, 60, 63, 67, 81, 83, 95, 88, 91, 75, 69,  71, 88, 70, 89, 76, 70, 93, 80, 79, 66, 72, 76, 76, 88, 69, 75,  60, 65, 77, 86, 80, 78, 74, 63, 85, 88, 69, 61, 73, 51, 88, 77,  65, 59, 64, 81, 73, 79, 61, 74, 69, 68, 71, 93, 64, 90, 69, 89,  63, 65, 67, 68, 63, 64, 77, 74, 66, 66, 82, 66, 71, 69, 58, 62,  61, 63, 42, 66, 64, 53, 70, 60, 56, 52, 59, 70, 64, 62, 76, 81,  63, 61, 76, 86, 63, 70, 88, 51, 68, 71, 69, 57, 63, 70, 64, 68,  72, 36, 50, 65, 71, 61, 61, 61, 70, 54, 52, 63, 70, 64, 59, 75,  63, 93, 73, 64, 70, 62, 65, 59, 58, 45, 66, 56, 74, 69, 53, 64,  54, 63, 63, 77, 74, 75, 61, 73, 70, 52, 61, 71, 47, 62, 62, 63,  65, 63, 63, 75, 72, 53, 72, 57, 72, 75, 75, 60, 73, 60, 61, 67,  48, 75, 82, 57, 63, 62, 42, 57, 69, 67, 64, 64, 50, 92, 62, 63,  75, 75, 67, 65, 58, 75, 75, 63, 62, 63, 67, 72, 67, 68, 69, 100,  75, 92, 78, 68, 83, 63, 78, 79, 75, 80, 73, 69, 75, 79, 78, 78,  86, 67, 66, 74, 54, 59, 57, 60, 62, 76, 83, 95, 93, 86, 67, 67,  67, 81, 67, 84, 68, 66, 86, 78, 75, 66, 72, 73, 69, 86, 67, 74,  61, 65, 84, 78, 73, 78, 64, 58, 83, 75, 66, 61, 68, 53, 92, 73,  61, 56, 69, 81, 71, 74, 55, 56, 69, 73, 65, 90, 60, 90, 68, 80,  63, 69, 60, 70, 64, 61, 71, 74, 60, 63, 71, 64, 75, 65, 58, 61,  59, 60, 39, 56, 58, 47, 57, 58, 51, 46, 62, 66, 58, 62, 71, 81,  63, 51, 67, 81, 56, 67, 86, 53, 69, 70, 67, 55, 58, 75, 66, 66,  63, 36, 47, 60, 69, 61, 64, 47, 70, 54, 55, 62, 69, 66, 55, 63,  60, 95, 71, 64, 68, 62, 64, 63, 54, 48, 70, 53, 70, 65, 55, 65,  50, 58, 63, 75, 71, 71, 64, 73, 68, 52, 55, 69, 51, 64, 53, 59,  65, 59, 63, 61, 61, 53, 69, 56, 62, 65, 58, 60, 72, 55, 63, 65,  54, 72, 82, 60, 52, 60, 42, 50, 75, 63, 64, 59, 50, 90, 53, 56,  69, 64, 66, 65, 61, 69, 69, 62, 60, 58, 62, 71, 66, 65, 66, 100,  74, 83, 73, 69, 81, 61, 83, 73, 73, 79, 68, 69, 73, 78, 75, 75,  85, 67, 63, 68, 51, 57, 57, 57, 64, 71, 79, 95, 92, 86, 67, 66,  63, 79, 67, 86, 60, 60, 86, 76, 73, 53, 68, 68, 61, 85, 65, 75,  59, 62, 74, 77, 70, 67, 58, 60, 70, 75, 65, 60, 65, 54, 83, 65,  65, 47, 61, 75, 68, 79, 59, 56, 65, 68, 69, 90, 59, 82, 66, 75,  65, 56, 58, 61, 65, 63, 67, 62, 59, 59, 72, 63, 71, 64, 60, 57,  59, 60, 46, 56, 60, 47, 50, 58, 54, 54, 60, 61, 58, 62, 65, 67,  61, 53, 66, 83, 59, 70, 87, 56, 70, 72, 67, 60, 65, 65, 64, 64,  72, 39, 53, 62, 67, 61, 66, 50, 68, 47, 52, 65, 64, 63, 57, 63,  67, 95, 77, 50, 67, 66, 65, 69, 56, 55, 68, 56, 68, 68, 52, 67,  52, 64, 66, 75, 75, 71, 75, 73, 70, 52, 59, 67, 50, 66, 58, 53,  60, 61, 63, 71, 64, 53, 72, 57, 63, 67, 69, 60, 67, 63, 63, 67,  48, 76, 80, 57, 56, 65, 56, 54, 69, 72, 66, 57, 55, 88, 45, 63,  72, 72, 66, 68, 50, 72, 71, 70, 63, 60, 67, 78, 58, 64, 67, 100,  74, 88, 77, 72, 88, 60, 82, 80, 68, 81, 70, 64, 82, 74, 81, 83,  85, 67, 66, 71, 52, 59, 59, 63, 65, 77, 83, 95, 95, 89, 63, 72,  67, 85, 65, 84, 63, 68, 83, 77, 71, 53, 67, 69, 60, 83, 65, 75,  63, 67, 77, 81, 72, 72, 60, 60, 67, 80, 63, 65, 67, 49, 79, 75,  69, 50, 61, 75, 68, 79, 60, 54, 65, 68, 65, 92, 65, 89, 66, 74,  68, 58, 65, 73, 72, 61, 63, 72, 51, 61, 80, 64, 61, 66, 54, 57,  52, 63, 49, 56, 62, 53, 52, 53, 52, 55, 60, 64, 61, 70, 68, 79,  64, 45, 68, 78, 61, 66, 85, 56, 65, 68, 67, 62, 62, 75, 55, 64,  63, 28, 53, 62, 69, 55, 64, 58, 65, 45, 52, 62, 58, 64, 55, 63,  63, 92, 79, 71, 62, 63, 63, 69, 54, 55, 65, 56, 70, 70, 48, 65,  48, 58, 65, 73, 72, 61, 57, 70, 68, 52, 57, 69, 41, 59, 55, 47,  55, 66, 63, 61, 64, 52, 69, 56, 62, 65, 77, 68, 62, 60, 65, 64,  48, 74, 78, 53, 56, 60, 53, 46, 69, 61, 64, 55, 50, 85, 60, 59,  75, 58, 64, 64, 47, 72, 71, 60, 67, 56, 62, 75, 64, 56, 67, 100,  76, 86, 76, 69, 75, 54, 78, 77, 71, 71, 68, 63, 77, 75, 78, 75,  82, 65, 61, 68, 50, 54, 58, 57, 60, 75, 75, 96, 92, 84, 56, 68,  63, 83, 67, 83, 60, 63, 87, 78, 69, 63, 64, 68, 65, 81, 69, 75,  61, 63, 74, 77, 70, 73, 52, 60, 72, 80, 63, 66, 58, 46, 75, 70,  57, 47, 61, 75, 71, 77, 53, 46, 68, 66, 60, 90, 64, 82, 65, 75,  69, 58, 63, 68, 63, 64, 67, 64, 52, 55, 76, 58, 64, 67, 58, 55,  52, 63, 41, 50, 63, 47, 50, 57, 57, 50, 60, 61, 61, 62, 71, 69,  59, 45, 66, 78, 61, 66), dim = c(224L, 6L), dimnames = list(c("X1",  "X3", "X4", "X5", "X8", "X12", "X13", "X14", "X16", "X17", "X18",  "X19", "X20", "X21", "X23", "X25", "X26", "X27", "X30", "X31",  "X32", "X33", "X34", "X35", "X36", "X39", "X40", "X41", "X42",  "X43", "X46", "X47", "X48", "X49", "X50", "X51", "X53", "X54",  "X55", "X56", "X57", "X58", "X61", "X63", "X64", "X66", "X69",  "X70", "X71", "X73", "X74", "X75", "X77", "X78", "X79", "X80",  "X82", "X83", "X84", "X86", "X87", "X88", "X90", "X92", "X93",  "X94", "X96", "X98", "X99", "X100", "X101", "X102", "X103", "X104",  "X106", "X107", "X112", "X113", "X115", "X116", "X117", "X118",  "X119", "X120", "X121", "X123", "X125", "X126", "X127", "X128",  "X129", "X132", "X133", "X134", "X136", "X137", "X138", "X139",  "X140", "X141", "X142", "X143", "X144", "X145", "X146", "X147",  "X149", "X150", "X152", "X153", "X155", "X160", "X161", "X162",  "X163", "X166", "X167", "X168", "X169", "X170", "X171", "X172",  "X173", "X174", "X176", "X177", "X181", "X182", "X183", "X184",  "X185", "X189", "X191", "X193", "X195", "X196", "X197", "X198",  "X200", "X201", "X202", "X203", "X204", "X205", "X206", "X207",  "X208", "X210", "X213", "X214", "X215", "X216", "X217", "X218",  "X219", "X221", "X222", "X223", "X224", "X226", "X228", "X229",  "X230", "X231", "X232", "X235", "X236", "X237", "X242", "X243",  "X244", "X245", "X246", "X247", "X249", "X250", "X251", "X252",  "X253", "X254", "X255", "X257", "X258", "X259", "X260", "X261",  "X262", "X263", "X265", "X266", "X270", "X271", "X272", "X274",  "X275", "X276", "X278", "X279", "X280", "X281", "X282", "X285",  "X287", "X289", "X290", "X291", "X292", "X295", "X296", "X297",  "X298", "X300", "X301", "X302", "X303", "X304", "X305", "X307",  "X310", "X311", "X312", "X314", "X315", "X316"), c("avg_score_Q10",  "avg_score_Q17", "avg_score_Q18", "avg_score_Q22", "avg_score_Q24",  "avg_score_Q29"))), y = c(`1` = 87.5, `3` = 56.5, `4` = 70, `5` = 71.5,  `8` = 70, `12` = 59.5, `13` = 67.25, `14` = 68.75, `16` = 59.75,  `17` = 65.25, `18` = 70.75, `19` = 31.5, `20` = 55.25, `21` = 63.5,  `23` = 72, `25` = 58.75, `26` = 65.25, `27` = 57.75, `30` = 69,  `31` = 49.25, `32` = 46.5, `33` = 66.75, `34` = 64.25, `35` = 61.5,  `36` = 60, `39` = 71.5, `40` = 64.5, `41` = 95, `42` = 74.75,  `43` = 59, `46` = 68.75, `47` = 62.75, `48` = 68, `49` = 67.5,  `50` = 54, `51` = 52.25, `53` = 63.75, `54` = 62, `55` = 72.25,  `56` = 66.5, `57` = 54, `58` = 65.75, `61` = 56.25, `63` = 64.5,  `64` = 62.5, `66` = 75.5, `69` = 77, `70` = 71.5, `71` = 60,  `73` = 73, `74` = 71.5, `75` = 52, `77` = 60.5, `78` = 71, `79` = 52.75,  `80` = 61.5, `82` = 57.5, `83` = 63.25, `84` = 64.75, `86` = 64.5,  `87` = 61, `88` = 65.25, `90` = 68, `92` = 54, `93` = 71.5, `94` = 56.75,  `96` = 68, `98` = 72.5, `99` = 74, `100` = 66, `101` = 70.25,  `102` = 57.25, `103` = 65.25, `104` = 66.75, `106` = 51.5, `107` = 77.5,  `112` = 81.25, `113` = 53.75, `115` = 56.5, `116` = 65.5, `117` = 51.5,  `118` = 54.5, `119` = 71.25, `120` = 65.25, `121` = 70.25, `123` = 59,  `125` = 56.75, `126` = 88, `127` = 63.25, `128` = 69.75, `129` = 75,  `132` = 72.25, `133` = 68.25, `134` = 70.25, `136` = 54.25, `137` = 73.5,  `138` = 73.5, `139` = 65.25, `140` = 65.75, `141` = 61.25, `142` = 68,  `143` = 72.75, `144` = 66.75, `145` = 65, `146` = 67.75, `147` = 100,  `149` = 75, `150` = 88.75, `152` = 77.5, `153` = 69.75, `155` = 83.75,  `160` = 69.25, `161` = 80, `162` = 78.25, `163` = 76.25, `166` = 77.5,  `167` = 69, `168` = 65.25, `169` = 74, `170` = 78.5, `171` = 79.25,  `172` = 80.5, `173` = 85.75, `174` = 66.75, `176` = 66.5, `177` = 72.5,  `181` = 52.5, `182` = 61.75, `183` = 59.5, `184` = 59.75, `185` = 62.75,  `189` = 77.25, `191` = 80, `193` = 95, `195` = 94.75, `196` = 88.5,  `197` = 63.5, `198` = 68.75, `200` = 66, `201` = 82.5, `202` = 66.25,  `203` = 85.75, `204` = 65, `205` = 68, `206` = 90.25, `207` = 79,  `208` = 70.5, `210` = 61.75, `213` = 68.25, `214` = 68.25, `215` = 73.25,  `216` = 83.25, `217` = 67.75, `218` = 75.5, `219` = 64, `221` = 63.5,  `222` = 78.75, `223` = 80.5, `224` = 73.25, `226` = 70.25, `228` = 60.25,  `229` = 60.75, `230` = 79, `231` = 79.25, `232` = 66.5, `235` = 66,  `236` = 68.25, `237` = 49.75, `242` = 81.5, `243` = 76.75, `244` = 62.5,  `245` = 56.25, `246` = 75.75, `247` = 76.5, `249` = 71.25, `250` = 75.5,  `251` = 59.75, `252` = 53.25, `253` = 66.75, `254` = 67, `255` = 63,  `257` = 91.5, `258` = 67.25, `259` = 86.75, `260` = 66, `261` = 75.75,  `262` = 65, `263` = 61.25, `265` = 62.25, `266` = 67.5, `270` = 66.5,  `271` = 63.25, `272` = 72, `274` = 72.25, `275` = 56.5, `276` = 63.75,  `278` = 78.75, `279` = 61.75, `280` = 68.75, `281` = 65.25, `282` = 58.75,  `285` = 59.75, `287` = 59.75, `289` = 60.75, `290` = 43.5, `291` = 53,  `292` = 61.75, `295` = 51.5, `296` = 47.75, `297` = 62.25, `298` = 55.25,  `300` = 50.75, `301` = 60.5, `302` = 64, `303` = 58.5, `304` = 65,  `305` = 74.25, `307` = 74, `310` = 60.5, `311` = 51.75, `312` = 68.25,  `314` = 79.75, `315` = 58.25, `316` = 69), family = "gaussian",      alpha = 0.1) 

   Df  %Dev Lambda
1   0  0.00 95.100
2   6  5.27 86.650
3   6 12.02 78.950
4   6 18.58 71.940
5   6 24.92 65.550
6   6 31.00 59.730
7   6 36.80 54.420
8   6 42.29 49.590
9   6 47.46 45.180
10  6 52.29 41.170
11  6 56.78 37.510
12  6 60.93 34.180
13  6 64.74 31.140
14  6 68.22 28.370
15  6 71.37 25.850
16  6 74.22 23.560
17  6 76.77 21.460
18  6 79.06 19.560
19  6 81.09 17.820
20  6 82.88 16.240
21  6 84.47 14.790
22  6 85.85 13.480
23  6 87.07 12.280
24  6 88.12 11.190
25  6 89.04 10.200
26  6 89.84  9.291
27  6 90.52  8.466
28  6 91.12  7.714
29  6 91.62  7.029
30  6 92.06  6.404
31  6 92.44  5.835
32  6 92.76  5.317
33  6 93.03  4.845
34  6 93.26  4.414
35  6 93.46  4.022
36  6 93.63  3.665
37  6 93.78  3.339
38  6 93.91  3.043
39  6 94.01  2.772
40  6 94.11  2.526
41  6 94.19  2.302
42  6 94.26  2.097
43  6 94.32  1.911
44  6 94.37  1.741
45  6 94.42  1.586
46  6 94.46  1.445
47  6 94.49  1.317
48  6 94.52  1.200
49  6 94.55  1.093
50  6 94.58  0.996
51  6 94.60  0.908
52  6 94.62  0.827
53  6 94.63  0.754
54  6 94.65  0.687
55  6 94.66  0.626
56  6 94.68  0.570
57  6 94.69  0.519
58  6 94.70  0.473
59  6 94.70  0.431
60  6 94.71  0.393
61  6 94.72  0.358
62  6 94.72  0.326
63  6 94.73  0.297
64  6 94.73  0.271
65  6 94.74  0.247
66  6 94.74  0.225
67  6 94.74  0.205
68  6 94.75  0.187
69  6 94.75  0.170
70  6 94.75  0.155
71  6 94.75  0.141
72  6 94.75  0.129
73  6 94.76  0.117
74  6 94.76  0.107
75  6 94.76  0.097
76  6 94.76  0.089
